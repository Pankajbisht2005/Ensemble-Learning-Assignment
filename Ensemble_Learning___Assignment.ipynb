{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "- Ensemble Learning\n",
        "\n",
        "Ensemble Learning is a machine learning technique in which multiple models (called base learners or weak learners) are trained and then combined to produce a single, stronger predictive model.\n",
        "\n",
        "Instead of relying on one model, ensemble learning aggregates the predictions of several models to improve:\n",
        "\n",
        "Accuracy\n",
        "\n",
        "Stability\n",
        "\n",
        "Generalization performance\n",
        "\n",
        " - Key Idea Behind Ensemble Learning\n",
        "\n",
        "The central principle is that a group of weak learners can be combined to form a strong learner.\n",
        "\n",
        " Weak Learner: A model that performs slightly better than random guessing.\n",
        "\n",
        " Strong Learner: A model with high accuracy and generalization ability.\n",
        "\n"
      ],
      "metadata": {
        "id": "8WDAYD8fUYow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "- Training method\n",
        "\n",
        " Bagging\n",
        "\n",
        "Models are trained independently\n",
        "\n",
        "Boosting\n",
        "\n",
        "Models are trained sequentially\n",
        "\n",
        "\n",
        "- Data sampling\n",
        "\n",
        "Bagging\n",
        "\n",
        "Uses bootstrap sampling (random sampling with replacement)\n",
        "\n",
        "Boosting\n",
        "\n",
        "Uses weighted data, increasing weight of misclassified points.\n",
        "\n",
        "\n",
        "- Focus\n",
        "\n",
        "Bagging\n",
        "\n",
        "Reduces variance\n",
        "\n",
        "Boosting\n",
        "\n",
        "Reduces bias (and variance in some cases)\n",
        "\n",
        "\n",
        "-Handling errors\n",
        "\n",
        "Bagging\n",
        "\n",
        "All data points treated equally.\n",
        "\n",
        "Boosting\n",
        "\n",
        "Misclassified points get more importance\n",
        "\n"
      ],
      "metadata": {
        "id": "dc0O-oqGUY2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "- Bootstrap sampling\n",
        "\n",
        "Bootstrap sampling is a resampling technique where multiple datasets are created by randomly sampling from the original dataset with replacement.\n",
        "\n",
        "- Role of Bootstrap Sampling in Bagging\n",
        "\n",
        "Bagging (Bootstrap Aggregating) depends on bootstrap sampling to:\n",
        "\n",
        "Create diversity among models\n",
        "Each model is trained on a different bootstrap sample.\n",
        "\n",
        "Reduce variance\n",
        "High-variance models (like decision trees) fluctuate heavily with small data changes. Bootstrapping smooths this instability.\n",
        "\n",
        "Enable effective aggregation\n",
        "Averaging uncorrelated models leads to better generalization.\n"
      ],
      "metadata": {
        "id": "tINRua0SUY6P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "\n",
        "- Out-of-Bag (OOB) samples\n",
        "\n",
        "Out-of-Bag (OOB) samples are the data points that are not selected in a bootstrap sample when training a model in bagging-based ensemble methods like Random Forest.\n",
        "\n",
        "The OOB score is used to evaluate the ensemble by:\n",
        "\n",
        "Predicting each data point using only the models for which that data point was OOB\n",
        "\n",
        "Comparing the predicted values with the actual values\n",
        "\n",
        "Computing a performance metric such as accuracy (classification) or mean squared error (regression)\n",
        "\n",
        "Thus, the OOB score provides an internal and unbiased estimate of model performance without requiring a separate validation dataset."
      ],
      "metadata": {
        "id": "MX6PpPPqXatR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "\n",
        "- Single Decision Tree\n",
        "\n",
        "In a single Decision Tree, feature importance is calculated based on how much each feature reduces impurity (such as Gini index or entropy) at the splits where it is used. Since the tree is built on the entire dataset, the importance values are unstable and can change significantly with small variations in data.\n",
        "\n",
        "- Randoms Forest\n",
        "\n",
        "In a Random Forest, feature importance is computed by averaging the impurity reduction contributed by each feature across all trees in the forest. Because multiple trees are trained on different bootstrap samples and feature subsets, the resulting feature importance is more stable, reliable, and less biased than that of a single decision tree.\n",
        "\n",
        "- Conclusion\n",
        "\n",
        "Feature importance from a single decision tree is highly sensitive to data, whereas Random Forest provides more robust and trustworthy feature importance due to aggregation across many trees."
      ],
      "metadata": {
        "id": "alrfka9DXard"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question 6: Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\"\"\"\n",
        "\n",
        "# Load Breast Cancer dataset, train Random Forest, and print top 5 important features\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "#  Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importances = rf.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better readability\n",
        "feature_df = pd.DataFrame({\n",
        "    'Feature': data.feature_names,\n",
        "    'Importance': feature_importances\n",
        "})\n",
        "\n",
        "# Sort features by importance and print top 5\n",
        "top_features = feature_df.sort_values(by='Importance', ascending=False).head(5)\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(top_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC0Nje18fHG7",
        "outputId": "2d100bd9-9eb7-4116-a00f-167e5f90a8ee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question 7: Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree\n",
        "(Include your Python code and output in the code box below.)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_predictions = dt.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
        "\n",
        "# Bagging Classifier using Decision Trees\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "bag_predictions = bagging.predict(X_test)\n",
        "bag_accuracy = accuracy_score(y_test, bag_predictions)\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", dt_accuracy)\n",
        "print(\"Bagging Classifier Accuracy:\", bag_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hek8WVErfHDd",
        "outputId": "959a36e9-768c-4b41-ada1-d7b124f927a7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question 8: Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy\n",
        "(Include your Python code and output in the code box below.)\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100, 200],\n",
        "    \"max_depth\": [None, 5, 10]\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"accuracy\"\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "# Final accuracy\n",
        "y_pred = best_model.predict(X_test)\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Final Accuracy:\", final_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3hU9sKTgDXx",
        "outputId": "63d80d62-8a7f-4317-e746-693ef6cd9825"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 200}\n",
            "Final Accuracy: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question 9: Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)\n",
        "(Include your Python code and output in the code box below.)\n",
        "\"\"\"\n",
        "\n",
        "# Bagging Regressor vs Random Forest Regressor on California Housing dataset\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 1: Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Step 2: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train Bagging Regressor (use 'estimator' instead of 'base_estimator')\n",
        "bagging_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "bagging_preds = bagging_reg.predict(X_test)\n",
        "bagging_mse = mean_squared_error(y_test, bagging_preds)\n",
        "\n",
        "# Step 4: Train Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_preds = rf_reg.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_preds)\n",
        "\n",
        "# Step 5: Print results\n",
        "print(\"Mean Squared Error (Bagging Regressor):\", bagging_mse)\n",
        "print(\"Mean Squared Error (Random Forest Regressor):\", rf_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lFzOyNMgDTt",
        "outputId": "7f407cb5-4b9a-4a68-adc7-79d663c0f5c6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Bagging Regressor): 0.25787382250585034\n",
            "Mean Squared Error (Random Forest Regressor): 0.25650512920799395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "\n",
        "● Choose between Bagging or Boosting\n",
        "\n",
        "● Handle overfitting\n",
        "\n",
        "● Select base models\n",
        "\n",
        "● Evaluate performance using cross-validation\n",
        "\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "\n",
        "- When I’m tasked with predicting loan defaults, I know the stakes are high because every wrong prediction can either hurt the bank financially or unfairly reject a customer. That’s why I’d lean on ensemble learning — it gives me a way to combine multiple models so that the final decision is more reliable than any single model.\n",
        "\n",
        "- Step 1: Choosing between Bagging and Boosting\n",
        "\n",
        "I’d start with Bagging (like Random Forests) because it’s stable and less sensitive to noise. It helps reduce variance, which is important when customer data can be messy. Once I have a strong baseline, I’d experiment with Boosting to see if it can capture subtle patterns in transaction history that Bagging might miss. In short, Bagging for safety, Boosting for sharper accuracy.\n",
        "\n",
        "- Step 2: Handling Overfitting\n",
        "\n",
        "Overfitting is a real risk in financial data. To control it, I’d keep tree depth limited, use regularization parameters, and monitor validation scores closely. I’d also make sure features are meaningful — removing irrelevant ones so the model doesn’t chase noise.\n",
        "\n",
        "\n",
        "- Step 3: Selecting Base Models\n",
        "Decision Trees are my go‑to base learners because they handle both numerical and categorical data well. For a more advanced ensemble like stacking, I might mix in logistic regression or gradient boosting trees to balance linear and non‑linear perspectives.\n",
        "\n",
        "\n",
        "- Step 4: Evaluating Performance with Cross‑Validation\n",
        "I’d use k‑fold cross‑validation to make sure the model performs consistently across different subsets of data. Since loan default is often imbalanced, I’d look at metrics like AUC‑ROC, precision, recall, and F1‑score rather than just accuracy. This ensures the model is genuinely useful in practice.\n",
        "\n",
        "\n",
        "- Step 5: Why Ensemble Learning Helps Here\n",
        "In the real world, ensemble learning is like having a panel of experts instead of one person making the call. Bagging reduces the risk of overfitting, Boosting sharpens accuracy, and together they give a balanced view. For the bank, this means fewer defaults slipping through and fewer good customers being wrongly rejected. It directly improves risk management and builds trust with customers.\n",
        "\n",
        "- CONCLUSION\n",
        "\n",
        "Ensemble learning makes loan default prediction smarter, safer, and fairer — exactly what a financial institution needs when decisions affect both money and people.\n"
      ],
      "metadata": {
        "id": "EPp3leSTXanQ"
      }
    }
  ]
}